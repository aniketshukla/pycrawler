__author__ = 'Aniket'
import __future__
import bs4
import urllib2 as donut
import socket
import sys
import os
Queue=list()
import sqlite3
robots=list() #global list of all the links to avoid
urlobject=list()
limit=0
while (1):
    try:
        con=sqlite3.connect('webcrawler')#yo ho pycache thanks for saving it in bytecode
        print ("database has been converted to bytecode")
        break
    except:
        print("trying again")
l=con.cursor()


class url:
    urlpath=None
    urlrank=None
    tempparentrank=None
    ip=None
    backlinks=None
    data=None
    urlproviderlist=list()
    urldirect=None
    pol=None

def table_existence():
    global l
    try:
        l.execute("SELECT * FROM  data")
        print(l)
        return 1
    except:
        return -1

def getlink (url):
    socket.setdefaulttimeout(2000)  #we don't want hosts to ban our ip's do we
    print("checker")
    html=donut.urlopen(url)
    html=html.read()
    html=html.replace("<scr'+'ipt","")#managing problem with python parser
    temp=list()
    soup=bs4.BeautifulSoup(html)
    for find in soup.find_all('a'):
        p=find.get('href')
        if (p.find('rel=nofollow')==-1):
            parts=donut.urlparse.urlsplit(p)
            if (parts.scheme!=''):
                temp.append(p)
        else:
            continue
    z=len(temp)
    return [[temp],[z]]

def add(queue,value):
    queue.append(value)

def pop(queue):
    return queue.pop(0)


#page rank algo is implemented in function database
def database(urlpath,urlparentrank,outlinks,urldirector,):
    print(urlpath)






fill=None

def page_mechanism(urlmain):
    global l
    global fill
    global q
    if fill==None:
        fill=0
        if table_existence()==-1:
            global l
            (l.execute('''CREATE TABLE data1
                      (urlpath VARCHAR(200)  PRIMARY KEY NOT NULL UNIQUE ,urlrank FLOAT  DESC not null ,backlinks DESC INT not null ,urldirectory blob )'''))
        link=url.urlpath
        urlobject.append(url())
        urlobject[fill].urlpath=urlmain
        urlobject[fill].tempparentrank=0
        urlobject[fill].backlinks=0
        urlobject[fill].pol=0
        add(Queue,urlobject[fill])
        print (urlobject[fill].urlpath)
    while (Queue!=None):
        print (1)
        linktemp=pop(Queue)
        rank=linktemp.urlrank
        print ("12",linktemp.urlpath)
        q1=getlink(linktemp.urlpath)
        len1=len(q1)
        print(len(linktemp.urlpath),linktemp.tempparentrank,linktemp.pol)
        temper=database(linktemp.urlpath,linktemp.tempparentrank,linktemp.pol,None)
        global limit
        limit=limit+1
        print(limit,"is this what we want")
        for i in q1[0][0]:
            fill=fill+1
            urlobject.append(url)
            urlobject[fill].urlpath=i
            urlobject[fill].tempparentrank=temper
            urlobject[fill].pol=len1
            add(Queue,urlobject[fill])



def main ():
    q=input("please enter link in quotation")
    page_mechanism(q)

main()





























